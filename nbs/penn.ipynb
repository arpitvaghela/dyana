{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for Penn TreeBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "import math\n",
    "from typing import Tuple, List, Union, Dict\n",
    "import numpy as np\n",
    "import grok\n",
    "from grok.training import TrainableTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and batch data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBIterator:\n",
    "    def __init__(self,train_pct, batchsize_hint, device,split=\"train\", data_dir=\"../data\",shuffle:bool = True) -> None:\n",
    "        self.device = device\n",
    "        # build vocab and tokenizer\n",
    "        train_iter = PennTreebank(root=data_dir, split=(\"train\"))\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "        \n",
    "        self.make_dataset(train_pct,batchsize_hint,data_dir, split)\n",
    "        self.reset_iteration(shuffle=shuffle)\n",
    "\n",
    "    def make_dataset(self,train_pct:float, batchsize, data_dir, split):\n",
    "\n",
    "        iter = PennTreebank(root=data_dir, split=(split))\n",
    "        dataset = self.batchify(self.data_process(iter), batchsize)\n",
    "        self.batchsize = batchsize\n",
    "        rows, _ = self.calc_split_len(train_pct, dataset.shape[0])\n",
    "        self.dataset = dataset[:rows]\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_batchsize(ds_size: int, batchsize_hint: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Calculates which batch size to use\n",
    "\n",
    "        :param ds_size: the number of equations in the dataset\n",
    "        :param batchsize_hint: * 0 means we use a default batchsize\n",
    "                               * -1 means the entire dataset\n",
    "                               * float between 0 and 1 means each batch is\n",
    "                                 that fraction of the DS\n",
    "                               * int > 1 means that specific batch size\n",
    "        :returns: the actual batchsize to use\n",
    "        \"\"\"\n",
    "\n",
    "        if batchsize_hint == -1:\n",
    "            return ds_size\n",
    "        elif batchsize_hint == 0:\n",
    "            return min(512, math.ceil(ds_size / 2.0))\n",
    "        elif (batchsize_hint > 0) and (batchsize_hint < 1):\n",
    "            return math.ceil(ds_size * batchsize_hint)\n",
    "        elif batchsize_hint > 1:\n",
    "            return min(batchsize_hint, ds_size)\n",
    "        else:\n",
    "            raise ValueError(\"batchsize_hint must be >= -1\")\n",
    "\n",
    "    def reset_iteration(self, shuffle=True):\n",
    "        self.index = 0\n",
    "        if shuffle:\n",
    "            self.permutation = torch.randperm(self.dataset.shape[0])\n",
    "        else:\n",
    "            self.permutation = torch.arange(self.dataset.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        :returns: this iterator\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Returns one batch of data.\n",
    "\n",
    "        :raises: StopIteration when we're out of data\n",
    "        :returns: batch tensor of shape (self.batchsize, tokens_per_eq)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_begin = self.index * self.batchsize\n",
    "        if batch_begin > len(self.dataset) - 1:\n",
    "            self.reset_iteration()\n",
    "            raise StopIteration\n",
    "        indices = self.permutation[batch_begin : batch_begin + self.batchsize]\n",
    "        text = self.dataset[indices, :-1]\n",
    "        target = self.dataset[indices, 1:]\n",
    "        batch = {\"text\": text.to(self.device), \"target\": target.to(self.device)}\n",
    "        self.index += 1\n",
    "        return batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        :returns: the total number of batches\n",
    "        \"\"\"\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def calc_split_len(self, train_pct, ds_len):\n",
    "        train_rows = round(ds_len * (train_pct / 100.0))\n",
    "        val_rows = ds_len - train_rows\n",
    "        return train_rows, val_rows\n",
    "\n",
    "    def data_process(self, raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "        \"\"\"Convert raw text to flat tensor\"\"\"\n",
    "        data = [torch.tensor(self.vocab(self.tokenizer(item))) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    def batchify(self, data, bsz):\n",
    "        print(data.shape[0])\n",
    "        seq_len = data.shape[0] // bsz\n",
    "        data = data[:seq_len * bsz]\n",
    "        return data.view(bsz, seq_len).contiguous()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': tensor([[2745,   97,  346,  ...,   50,    6,   61],\n",
       "         [   2,    4,  182,  ...,    6,    1,  803],\n",
       "         [1567,    7, 5504,  ...,    2,    7,    2],\n",
       "         ...,\n",
       "         [ 110, 9203,  747,  ...,   77,   12,    1],\n",
       "         [1925,    3,    2,  ..., 1086,   15, 6607],\n",
       "         [  31,  871,    0,  ...,   34,   26,  972]]),\n",
       " 'target': tensor([[  97,  346,    4,  ...,    6,   61,  746],\n",
       "         [   4,  182,   38,  ...,    1,  803,  277],\n",
       "         [   7, 5504, 4465,  ...,    7,    2,    2],\n",
       "         ...,\n",
       "         [9203,  747,   52,  ...,   12,    1,  162],\n",
       "         [   3,    2,  171,  ...,   15, 6607,    1],\n",
       "         [ 871,    0, 1192,  ...,   26,  972,   12]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = PTBIterator(50,100,None)\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': tensor([[  0,  73,   6, 107,   1,  61],\n",
       "         [  0,  81,   6, 103,   1,  65],\n",
       "         [  0,  51,   6,  58,   1,  87],\n",
       "         [  0,  70,   6,  98,   1,  49],\n",
       "         [  0,  56,   6, 100,   1,  37],\n",
       "         [  0,  40,   6,  88,   1, 106],\n",
       "         [  0, 118,   6, 101,   1, 100],\n",
       "         [  0,  43,   6,  66,   1,  87],\n",
       "         [  0,  73,   6, 113,   1,  67],\n",
       "         [  0,  29,   6,  26,   1,  33],\n",
       "         [  0, 118,   6,  68,   1,  67],\n",
       "         [  0,  31,   6,  87,   1,  96]]),\n",
       " 'target': tensor([[ 73,   6, 107,   1,  61,   0],\n",
       "         [ 81,   6, 103,   1,  65,   0],\n",
       "         [ 51,   6,  58,   1,  87,   0],\n",
       "         [ 70,   6,  98,   1,  49,   0],\n",
       "         [ 56,   6, 100,   1,  37,   0],\n",
       "         [ 40,   6,  88,   1, 106,   0],\n",
       "         [118,   6, 101,   1, 100,   0],\n",
       "         [ 43,   6,  66,   1,  87,   0],\n",
       "         [ 73,   6, 113,   1,  67,   0],\n",
       "         [ 29,   6,  26,   1,  33,   0],\n",
       "         [118,   6,  68,   1,  67,   0],\n",
       "         [ 31,   6,  87,   1,  96,   0]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, _ =  grok.data.ArithmeticDataset.splits(5, \"+\")\n",
    "iterator = grok.data.ArithmeticIterator(\n",
    "            train_dataset,\n",
    "            None,\n",
    "            batchsize_hint=12,  # type: ignore\n",
    "        )\n",
    "d=  next(iter(iterator))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1320 batches | lr 5.00 | ms/batch 255.35 | loss  7.25 | ppl  1401.81\n",
      "| epoch   1 |   400/ 1320 batches | lr 5.00 | ms/batch 157.64 | loss  6.12 | ppl   454.28\n",
      "| epoch   1 |   600/ 1320 batches | lr 5.00 | ms/batch 157.42 | loss  5.87 | ppl   352.69\n",
      "| epoch   1 |   800/ 1320 batches | lr 5.00 | ms/batch 188.83 | loss  5.68 | ppl   292.56\n",
      "| epoch   1 |  1000/ 1320 batches | lr 5.00 | ms/batch 152.71 | loss  5.60 | ppl   270.14\n",
      "| epoch   1 |  1200/ 1320 batches | lr 5.00 | ms/batch 161.84 | loss  5.49 | ppl   243.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 251.30s | valid loss  5.49 | valid ppl   242.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1320 batches | lr 4.75 | ms/batch 116.82 | loss  5.43 | ppl   228.55\n",
      "| epoch   2 |   400/ 1320 batches | lr 4.75 | ms/batch 109.64 | loss  5.37 | ppl   215.61\n",
      "| epoch   2 |   600/ 1320 batches | lr 4.75 | ms/batch 108.82 | loss  5.34 | ppl   207.92\n",
      "| epoch   2 |   800/ 1320 batches | lr 4.75 | ms/batch 114.49 | loss  5.27 | ppl   194.22\n",
      "| epoch   2 |  1000/ 1320 batches | lr 4.75 | ms/batch 156.13 | loss  5.27 | ppl   194.76\n",
      "| epoch   2 |  1200/ 1320 batches | lr 4.75 | ms/batch 141.59 | loss  5.19 | ppl   179.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 171.50s | valid loss  5.31 | valid ppl   202.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1320 batches | lr 4.51 | ms/batch 141.95 | loss  5.19 | ppl   179.45\n",
      "| epoch   3 |   400/ 1320 batches | lr 4.51 | ms/batch 141.79 | loss  5.16 | ppl   173.86\n",
      "| epoch   3 |   600/ 1320 batches | lr 4.51 | ms/batch 143.10 | loss  5.13 | ppl   169.03\n",
      "| epoch   3 |   800/ 1320 batches | lr 4.51 | ms/batch 145.40 | loss  5.08 | ppl   161.00\n",
      "| epoch   3 |  1000/ 1320 batches | lr 4.51 | ms/batch 137.74 | loss  5.10 | ppl   164.60\n",
      "| epoch   3 |  1200/ 1320 batches | lr 4.51 | ms/batch 140.35 | loss  5.02 | ppl   151.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 190.69s | valid loss  5.27 | valid ppl   194.22\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.23 | test ppl   186.91\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
