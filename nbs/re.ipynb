{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking : Generalization beyond Overfitting on Small Dataset\n",
    "\n",
    "This Notebook aims at exploring the official implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "import itertools\n",
    "import time\n",
    "import grok\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grok.data import ArithmeticDataset, ArithmeticTokenizer\n",
    "\n",
    "train_ds, valid_ds = ArithmeticDataset.splits(50, \"s5\", None, \"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 120 ** 2 == train_ds.data.shape[0] + valid_ds.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 7]),\n",
       " ['<|eos|> 21304 s5 12430 = 42310 <|eos|>',\n",
       "  '<|eos|> 34102 s5 10243 = 43012 <|eos|>',\n",
       "  '<|eos|> 03124 s5 31420 = 32140 <|eos|>',\n",
       "  '<|eos|> 42130 s5 41203 = 32104 <|eos|>',\n",
       "  '<|eos|> 40123 s5 03142 = 20314 <|eos|>'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data[:5].shape, train_ds.tokenizer.decode(train_ds.data[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grok.transformer import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Transformer(n_layers=2, d_model=32, n_heads=4).float()\n",
    "net2 = Transformer(n_layers=2, d_model=33, n_heads=4).float()\n",
    "net.n_heads, net.n_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.0.Wq.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.0.Wk.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.0.Wv.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.1.Wq.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.1.Wk.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.1.Wv.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.2.Wq.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.2.Wk.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.2.Wv.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.3.Wq.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.3.Wk.weight\n",
      "decoder.blocks.0.self_attn.attn_heads.3.Wv.weight\n",
      "decoder.blocks.0.self_attn.Wo.weight\n",
      "decoder.blocks.0.self_attn_norm.weight\n",
      "decoder.blocks.0.self_attn_norm.bias\n",
      "decoder.blocks.0.ffn.ffn.0.weight\n",
      "decoder.blocks.0.ffn.ffn.2.weight\n",
      "decoder.blocks.0.ffn_norm.weight\n",
      "decoder.blocks.0.ffn_norm.bias\n",
      "decoder.blocks.1.self_attn.attn_heads.0.Wq.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.0.Wk.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.0.Wv.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.1.Wq.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.1.Wk.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.1.Wv.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.2.Wq.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.2.Wk.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.2.Wv.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.3.Wq.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.3.Wk.weight\n",
      "decoder.blocks.1.self_attn.attn_heads.3.Wv.weight\n",
      "decoder.blocks.1.self_attn.Wo.weight\n",
      "decoder.blocks.1.self_attn_norm.weight\n",
      "decoder.blocks.1.self_attn_norm.bias\n",
      "decoder.blocks.1.ffn.ffn.0.weight\n",
      "decoder.blocks.1.ffn.ffn.2.weight\n",
      "decoder.blocks.1.ffn_norm.weight\n",
      "decoder.blocks.1.ffn_norm.bias\n",
      "linear.weight\n"
     ]
    }
   ],
   "source": [
    "exp_method = \"zero\"\n",
    "for (k1,p1),(k2,p2) in zip(net.named_parameters(), net2.named_parameters()):\n",
    "    assert k1 == k2\n",
    "    print(k1)\n",
    "    #print(p1.shape, p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4, 2, 4, 0, 0, 1],\n",
       "        [0, 1, 4, 1, 3, 0, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.randint(5, [2, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3065,  0.5112,  0.0829,  ..., -0.0302,  0.2889,  0.0773],\n",
       "          [-0.1615,  0.5361,  0.0981,  ..., -0.1725,  0.4206, -0.0861],\n",
       "          [ 0.7925, -1.1406,  0.1976,  ...,  0.1937,  0.5750,  1.0105],\n",
       "          ...,\n",
       "          [ 0.9616,  0.3109,  0.2930,  ..., -0.9916, -0.0333, -0.0638],\n",
       "          [ 0.2606, -0.4889,  0.5550,  ...,  0.2374,  0.6205,  0.7361],\n",
       "          [-0.1961,  1.0642,  0.3349,  ..., -0.5135,  0.6176, -0.4522]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(th.randint(5, [1, 7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = th.randn([5, 2])\n",
    "t2 = th.randn([7, 5])\n",
    "t_ = t1.clone()\n",
    "for dim in range(len(t2.shape)):\n",
    "    m = t2.shape[dim] - t1.shape[dim]\n",
    "    idx = th.tensor(np.random.choice(range(t1.shape[dim]), size=m, replace=True))\n",
    "    m_ = th.index_select(t_, dim, idx)\n",
    "    t_ = th.cat((t_, m_), dim=dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_model(\n",
    "    teacher_model: grok.transformer.Transformer,\n",
    "    add_dmodel: int,\n",
    "    exp_method: str = \"random\",\n",
    ") -> grok.transformer.Transformer:\n",
    "    \"\"\"Expand a Transformer to a multiple of its size.\n",
    "\n",
    "    Args:\n",
    "        parent_net:(grok.transformer.Transformer) The parent model to expand from.\n",
    "        add_dmodel:(int) increase in the size of d_model.\n",
    "        exp_method:(str) [duplicate | random | zero] Method used to initialize new parameter.\n",
    "\n",
    "    Returns:\n",
    "        student_model: (grok.transformer.Transformer) The new transformer with d_model = parent_model.d_model + add_dmodel\n",
    "    \"\"\"\n",
    "    params1 = teacher_model.state_dict()\n",
    "    student_model = type(teacher_model)(\n",
    "        n_layers=teacher_model.n_layers,\n",
    "        n_heads=teacher_model.n_heads,\n",
    "        d_model=teacher_model.d_model + add_dmodel,\n",
    "    )\n",
    "    params2 = student_model.state_dict()\n",
    "\n",
    "    assert exp_method in [\"duplicate\", \"random\", \"zero\"], \"Invalid expansion method.\"\n",
    "    \n",
    "    return student_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(2000, 16)\n",
      "  (decoder): Decoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (attn_heads): ModuleList(\n",
      "            (0): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (1): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (2): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (3): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "          )\n",
      "          (Wo): Linear(in_features=16, out_features=16, bias=False)\n",
      "        )\n",
      "        (self_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=False)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (ffn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (ffn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (attn_heads): ModuleList(\n",
      "            (0): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (1): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (2): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (3): AttentionHead(\n",
      "              (Wq): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wk): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Wv): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "          )\n",
      "          (Wo): Linear(in_features=16, out_features=16, bias=False)\n",
      "        )\n",
      "        (self_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=False)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (ffn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (ffn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=16, out_features=2000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(2000, 34)\n",
       "  (decoder): Decoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (attn_heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=34, out_features=34, bias=False)\n",
       "        )\n",
       "        (self_attn_norm): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=34, out_features=136, bias=False)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=136, out_features=34, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (ffn_norm): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (attn_heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Wq): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wk): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (Wv): Linear(in_features=34, out_features=8, bias=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (Wo): Linear(in_features=34, out_features=34, bias=False)\n",
       "        )\n",
       "        (self_attn_norm): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=34, out_features=136, bias=False)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=136, out_features=34, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (ffn_norm): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=34, out_features=2000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = Transformer(n_heads=4, n_layers=2, d_model=64)\n",
    "print(type(net2)(n_heads=4, n_layers=2, d_model=16))\n",
    "expand_model(net, 2, exp_method=\"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableExpTransformer(pl.LightningModule):\n",
    "    def __init__(self, n_layers=2, d_model=4, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.init_n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.init_n_heads = n_heads\n",
    "        self.transformer = Transformer(\n",
    "            n_layers=self.init_n_layers,\n",
    "            d_model=self.d_model,\n",
    "            n_heads=self.init_n_heads,\n",
    "        ).float()\n",
    "\n",
    "    def expand_model(\n",
    "        self,\n",
    "        add_dmodel: int,\n",
    "        exp_method: str = \"random\",\n",
    "    ) -> None:\n",
    "        \"\"\"Expand Transformer dmodel to dmodel + add_dmodel.\n",
    "\n",
    "        Args:\n",
    "            parent_net:(grok.transformer.Transformer) The parent model to expand from.\n",
    "            add_dmodel:(int) increase in the size of d_model.\n",
    "            exp_method:(str) [duplicate | random | zero] Method used to initialize new parameter.\n",
    "        \"\"\"\n",
    "        print(f\"\\nExpanding to size {self.d_model*add_dmodel}\")\n",
    "\n",
    "        teacher_model = self.transformer\n",
    "        params1 = teacher_model.state_dict()\n",
    "        student_model = type(teacher_model)(\n",
    "            n_layers=teacher_model.n_layers,\n",
    "            n_heads=teacher_model.n_heads,\n",
    "            d_model=teacher_model.d_model + add_dmodel,\n",
    "        )\n",
    "        params2 = student_model.state_dict()\n",
    "\n",
    "        assert exp_method in [\n",
    "            \"duplicate\",\n",
    "            \"random\",\n",
    "            \"zero\",\n",
    "        ], \"Invalid expansion method.\"\n",
    "        params_new = {}\n",
    "        for k in params2:\n",
    "            if k == \"self_attn_mask\":\n",
    "                params_new.update({k: params2[k].clone()})\n",
    "\n",
    "            elif params2[k].shape == params1[k].shape:\n",
    "                params_new.update({k: params1[k].clone()})\n",
    "            else:\n",
    "                new_shape = params2[k].shape\n",
    "                old_shape = params1[k].shape\n",
    "                w_ = params1[k].clone()\n",
    "                for dim in range(len(new_shape)):\n",
    "                    # m is the size  to concat in dimension `dim``\n",
    "                    m = new_shape[dim] - old_shape[dim]\n",
    "                    if exp_method == \"duplicate\":\n",
    "                        idx = th.tensor(\n",
    "                            np.random.choice(range(t1.shape[dim]), size=m, replace=True)\n",
    "                        )\n",
    "                        v_ = th.index_select(w_, dim, idx)\n",
    "                        w_ = th.cat((w_, v_), dim=dim)\n",
    "\n",
    "                    elif exp_method == \"random\":\n",
    "                        shape_of_exta = w_.shape[:dim] + (m,) + w_.shape[dim + 1 :]\n",
    "                        v_ = th.randn(shape_of_exta)\n",
    "                        w_ = th.cat((w_, v_), dim=dim)\n",
    "\n",
    "                    elif exp_method == \"zero\":\n",
    "                        m = new_shape[dim] - old_shape[dim]\n",
    "                        shape_of_exta = w_.shape[:dim] + (m,) + w_.shape[dim + 1 :]\n",
    "                        v_ = th.zeros(shape_of_exta)\n",
    "                        w_ = th.cat((w_, v_), dim=dim)\n",
    "\n",
    "                params_new.update({k: w_})\n",
    "        student_model.load_state_dict(params_new)\n",
    "        self.transformer = student_model.float()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Passes all arguments directly to Tranformer.forward()\"\"\"\n",
    "        return self.transformer(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if batch_idx == 0:\n",
    "            self.training_epoch_start_time = time.time()\n",
    "            self.fwd_time_in_epoch = 0\n",
    "\n",
    "        start = time.time()\n",
    "        loss = self._step(batch=batch, batch_idx=batch_idx, train=True)\n",
    "        self.fwd_time_in_epoch += time.time() - start\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _step(\n",
    "        self, batch: Dict, batch_idx: int, train: bool = True, reduction: str = \"mean\"\n",
    "    ) -> Tuple[th.Tensor, th.Tensor, float, th.Tensor, th.Tensor, th.Tensor, th.Tensor]:\n",
    "        x, y = batch  # shape = batchsize * context_len\n",
    "        y_hat, attentions, values = self(\n",
    "            x=x\n",
    "        )  # shape = batchsize * context_len * vocab_size\n",
    "        y_hat = y_hat.transpose(-2, -1)  # shape = batchsize * vocab_size * context_len\n",
    "        loss = F.cross_entropy(y_hat, y, reduction=reduction)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Used by pytorch_lighting\n",
    "\n",
    "        :returns: optimizers and schedulers.\n",
    "        \"\"\"\n",
    "        optimizer = th.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-8,\n",
    "            lr=1,\n",
    "        )\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandModelCallback(pl.callbacks.Callback):\n",
    "    def on_epoch_end(self, trainer: pl.Trainer, pl_module: TrainableExpTransformer):\n",
    "        N = trainer.max_epochs / 4\n",
    "        if pl_module.current_epoch > 0 and pl_module.current_epoch % N == 0:\n",
    "            pl_module.expand_model(8, exp_method=\"zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name        | Type        | Params\n",
      "--------------------------------------------\n",
      "0 | transformer | Transformer | 16.4 K\n",
      "--------------------------------------------\n",
      "16.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.4 K    Total params\n",
      "0.066     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s, loss=0.000158, v_num=29]Expanding model\n",
      "\n",
      "Expanding to size 32\n",
      "Epoch 436:   0%|          | 0/1 [00:00<00:00, 1209.43it/s, loss=0.000332, v_num=29] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpha91/miniconda3/envs/syft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1047: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "dataset = th.randint(100, [1024, 7])\n",
    "data = dataset[:, :-1]\n",
    "target = dataset[:, 1:]\n",
    "dataloader = th.utils.data.DataLoader((data, target), batch_size=512)\n",
    "\n",
    "model = TrainableExpTransformer()\n",
    "trainer = pl.Trainer(max_epochs=1000, callbacks=[ExpandModelCallback()])\n",
    "trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11b01b9c5b8ff60b99e90016c0fc35672e8bff0e840ae7e3fc812494c63e782d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('syft': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
