{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from numpy import cos, sin\n",
    "import numpy as np\n",
    "from grok.transformer import MultiHeadAttention, LayerNorm, FFN, Transformer\n",
    "from typing import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1377, 0.7205, 0.4607],\n",
       "        [0.1675, 0.6693, 0.7980],\n",
       "        [0.7938, 0.8521, 0.8825],\n",
       "        [0.0472, 0.5890, 0.8053]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = 100\n",
    "x = th.tensor([5,11,6, 66])\n",
    "embedding_weight = th.rand((vocab_len,3))\n",
    "F.embedding(x, embedding_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[slice(None, 100, None), slice(None, 3, None)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1377, 0.7205, 0.4607, 0.0000],\n",
       "        [0.1675, 0.6693, 0.7980, 0.0000],\n",
       "        [0.7938, 0.8521, 0.8825, 0.0000],\n",
       "        [0.0472, 0.5890, 0.8053, 0.0000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_exp = th.zeros((vocab_len,4))\n",
    "size = embedding_weight.shape\n",
    "size = [slice(x) for x in size]\n",
    "print(size)\n",
    "embedding_weight_exp[size] = embedding_weight\n",
    "F.embedding(x, embedding_weight_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Embedding Layer with positional encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_position_encoding(context_len: int, d_model: int) -> th.Tensor:\n",
    "        rows = [\n",
    "            th.tensor(\n",
    "                [\n",
    "                    sin(pos / (10000 ** (i / d_model)))\n",
    "                    if i % 2 == 0\n",
    "                    else cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "                    for i in range(d_model)\n",
    "                ]\n",
    "            )\n",
    "            for pos in range(context_len)\n",
    "        ]\n",
    "        stack = th.stack(rows, dim=1)\n",
    "\n",
    "        return stack.T  # type: ignore\n",
    "gen_position_encoding(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(indices: th.Tensor, embedding_weight:th.Tensor, position_encoding:th.Tensor) -> th.Tensor:\n",
    "        context_len = indices.shape[-1]\n",
    "        pe = position_encoding[:context_len, :]  # type: ignore\n",
    "        embedded = F.embedding(indices,embedding_weight)\n",
    "        return pe + embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1377,  1.7205,  0.4607],\n",
       "        [ 1.0090,  1.2096,  0.8002],\n",
       "        [ 1.7031,  0.4359,  0.8868],\n",
       "        [ 0.1883, -0.4010,  0.8117]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(x, embedding_weight, gen_position_encoding(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1377,  1.7205,  0.4607,  1.0000],\n",
       "        [ 1.0090,  1.2096,  0.8080,  1.0000],\n",
       "        [ 1.7031,  0.4359,  0.9025,  0.9998],\n",
       "        [ 0.1883, -0.4010,  0.8353,  0.9996]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(x, embedding_weight_exp, gen_position_encoding(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Expansion logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_emb(self, i):\n",
    "    return self.embedding(i)\n",
    "\n",
    "Transformer.embed = new_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Transformer(n_layers=1, n_heads=3, d_model=12)\n",
    "net1.d_model//net1.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(net1, \"./checkpoints/net1.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_transfer(net2:th.nn.Module, old_state_path:str):\n",
    "    net1 = th.load(old_state_path)\n",
    "    old_state = net1.state_dict()\n",
    "    n_layers_old = net1.n_layers\n",
    "    n_head_old = net1.n_heads\n",
    "\n",
    "    dk_old = net1.d_model//net1.n_heads\n",
    "    dk_new = net2.d_model//net2.n_heads\n",
    "\n",
    "    new_state = net2.state_dict() \n",
    "    updated_state = deepcopy(new_state)\n",
    "    for k in new_state:\n",
    "        # print(k)\n",
    "        if k == \"position_encoding\" or k == \"self_attn_mask\":\n",
    "            continue\n",
    "        elif \"self_attn_norm\" in k.split(\".\") or \"ffn_norm\" in k.split(\".\"):\n",
    "            continue\n",
    "        elif \"attn_heads\" in k.split(\".\"):\n",
    "            updated_state[k] = th.zeros_like(new_state[k])\n",
    "            weight_name = k.split(\".\")\n",
    "            layer_idx = int(weight_name[2])\n",
    "            if layer_idx < n_layers_old:\n",
    "                head_idx = int(weight_name[5])   \n",
    "                lst = [(i//dk_old, i%dk_old) for i in (head_idx*dk_new, head_idx*dk_new +dk_new)]\n",
    "                w = []\n",
    "                if lst[0][0] == lst[1][0]:\n",
    "                    w.append(old_state[k][ lst[0][1]: lst[1][1], :])\n",
    "                else:\n",
    "                    for prev_head_idx in range(lst[0][0], lst[1][0]+1):\n",
    "                        if not prev_head_idx < n_head_old:\n",
    "                            continue\n",
    "                        weight_name_old = weight_name.copy()\n",
    "                        weight_name_old[5] = str(prev_head_idx)\n",
    "                        k_old = \".\".join(weight_name_old)\n",
    "\n",
    "                        if prev_head_idx == lst[0][0]:\n",
    "                            w_dash = old_state[k_old][lst[0][1]: , :]\n",
    "                            # print(rng,w_dash.shape)\n",
    "                            w.append(w_dash)\n",
    "\n",
    "                        elif prev_head_idx == lst[1][0]:\n",
    "                            w_dash = old_state[k_old][ :lst[1][1], :]\n",
    "                            # print(rng, w_dash.shape)\n",
    "                            w.append(w_dash)\n",
    "                        else:\n",
    "                            w.append(old_state[k_old])\n",
    "                    if w:\n",
    "                        final_old_w = th.cat(w)\n",
    "                        dice = [slice(dim) for dim in final_old_w.shape]\n",
    "                        updated_state[k][dice] = final_old_w\n",
    "        else:\n",
    "            updated_state[k] = th.zeros_like(new_state[k])\n",
    "            if k in old_state:\n",
    "                dice = [slice(dim) for dim in old_state[k].shape]\n",
    "                updated_state[k][dice] = old_state[k]\n",
    "        \n",
    "    net2.load_state_dict(updated_state)\n",
    "\n",
    "net2 = Transformer(n_layers=2, n_heads=4, d_model=16)\n",
    "knowledge_transfer(net2, \"./checkpoints/net1.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in updated_state:\n",
    "#     print(k, updated_state[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6082, -1.1562, -0.4722, -0.1791, -0.6773, -0.7933, -0.5872, -0.8640,\n",
       "           0.3931, -0.2593, -0.6099, -1.1041]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.6082, -1.1562, -0.4722, -0.1791, -0.6773, -0.7933, -0.5872, -0.8640,\n",
       "           0.3931, -0.2593, -0.6099, -1.1041,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor([11])\n",
    "em1 = net1.embed(x)\n",
    "em2 = net2.embed(x)\n",
    "em1, em2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.2708, -0.0279,  0.2348, -0.1655,  0.0838,  0.4526,  0.1671, -0.2010,\n",
       "           -0.0152, -0.0036,  0.0667, -0.0103]], grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []),\n",
       " (tensor([[-0.2708, -0.0279,  0.2348, -0.1655,  0.0838,  0.4526,  0.1671, -0.2010,\n",
       "           -0.0152, -0.0036,  0.0667, -0.0103,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "         grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.decoder.blocks[0].self_attn(em1,em1,em1), net2.decoder.blocks[0].self_attn(em2,em2,em2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.9175, -1.2313,  0.6247,  0.4132,  0.1563,  0.8593,  0.1814, -1.0971,\n",
       "            2.1925,  0.4381, -0.1889, -1.4307]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),\n",
       "  [],\n",
       "  []),\n",
       " (tensor([[-1.1534, -1.4880,  0.3027,  0.1785, -0.1695,  0.5351,  0.0983, -1.5038,\n",
       "            1.8673,  0.2039, -0.5342, -1.7418,  0.8512,  0.8512,  0.8512,  0.8512]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.decoder.blocks[0](em1), net2.decoder.blocks[0](em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6082, -1.1562, -0.4722, -0.1791, -0.6773, -0.7933, -0.5872, -0.8640,\n",
       "           0.3931, -0.2593, -0.6099, -1.1041,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " (tensor([[-0.4085, -1.6820, -0.0927,  0.5884, -0.5691, -0.8388, -0.3598, -1.0030,\n",
       "            1.9183,  0.4021, -0.4127, -1.5609,  1.0047,  1.0047,  1.0047,  1.0047]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em2, net2.decoder.blocks[1](em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.1523, -0.0998,  0.0358,  ...,  0.0609, -0.1605,  0.0148]],\n",
       "         grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []),\n",
       " (tensor([[-0.3205, -0.1772, -0.0632,  ...,  0.1257, -0.1621, -0.3319]],\n",
       "         grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1(x), net2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = th.nn.LayerNorm(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6082, -1.1562, -0.4722, -0.1791, -0.6773, -0.7933, -0.5872, -0.8640,\n",
       "           0.3931, -0.2593, -0.6099, -1.1041,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.4085, -1.6820, -0.0927,  0.5884, -0.5691, -0.8388, -0.3598, -1.0030,\n",
       "           1.9182,  0.4021, -0.4127, -1.5609,  1.0047,  1.0047,  1.0047,  1.0047]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em2, norm(em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.decoder.blocks[0].self_attn_norm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16967/2350051098.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_w' is not defined"
     ]
    }
   ],
   "source": [
    "em2, F.layer_norm(em2, (16,), norm_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11b01b9c5b8ff60b99e90016c0fc35672e8bff0e840ae7e3fc812494c63e782d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
