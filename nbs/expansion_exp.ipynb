{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from numpy import cos, sin\n",
    "import numpy as np\n",
    "from grok.transformer import MultiHeadAttention, LayerNorm, FFN, Transformer\n",
    "from typing import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7974, 0.7866, 0.2839],\n",
       "        [0.6512, 0.3038, 0.4062],\n",
       "        [0.5381, 0.4823, 0.1273],\n",
       "        [0.4329, 0.4239, 0.8615]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = 100\n",
    "x = th.tensor([5,11,6, 66])\n",
    "embedding_weight = th.rand((vocab_len,3))\n",
    "F.embedding(x, embedding_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[slice(None, 100, None), slice(None, 3, None)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7974, 0.7866, 0.2839, 0.0000],\n",
       "        [0.6512, 0.3038, 0.4062, 0.0000],\n",
       "        [0.5381, 0.4823, 0.1273, 0.0000],\n",
       "        [0.4329, 0.4239, 0.8615, 0.0000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_exp = th.zeros((vocab_len,4))\n",
    "size = embedding_weight.shape\n",
    "size = [slice(x) for x in size]\n",
    "print(size)\n",
    "embedding_weight_exp[size] = embedding_weight\n",
    "F.embedding(x, embedding_weight_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Embedding Layer with positional encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_position_encoding(context_len: int, d_model: int) -> th.Tensor:\n",
    "        rows = [\n",
    "            th.tensor(\n",
    "                [\n",
    "                    sin(pos / (10000 ** (i / d_model)))\n",
    "                    if i % 2 == 0\n",
    "                    else cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "                    for i in range(d_model)\n",
    "                ]\n",
    "            )\n",
    "            for pos in range(context_len)\n",
    "        ]\n",
    "        stack = th.stack(rows, dim=1)\n",
    "\n",
    "        return stack.T  # type: ignore\n",
    "gen_position_encoding(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(indices: th.Tensor, embedding_weight:th.Tensor, position_encoding:th.Tensor) -> th.Tensor:\n",
    "        context_len = indices.shape[-1]\n",
    "        pe = position_encoding[:context_len, :]  # type: ignore\n",
    "        embedded = F.embedding(indices,embedding_weight)\n",
    "        return pe + embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7974,  1.7866,  0.2839],\n",
       "        [ 1.4927,  0.8441,  0.4083],\n",
       "        [ 1.4474,  0.0662,  0.1317],\n",
       "        [ 0.5740, -0.5661,  0.8680]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(x, embedding_weight, gen_position_encoding(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7974,  1.7866,  0.2839,  1.0000],\n",
       "        [ 1.4927,  0.8441,  0.4162,  1.0000],\n",
       "        [ 1.4474,  0.0662,  0.1473,  0.9998],\n",
       "        [ 0.5740, -0.5661,  0.8915,  0.9996]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(x, embedding_weight_exp, gen_position_encoding(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Expansion logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_emb(self, i):\n",
    "    return self.embedding(i)\n",
    "\n",
    "Transformer.embed = new_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Transformer(n_layers=1, n_heads=3, d_model=12)\n",
    "net1.d_model//net1.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(net1, \"./checkpoints/net1.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_transfer(net2:th.nn.Module, old_state_path:str):\n",
    "    net1 = th.load(old_state_path)\n",
    "    old_state = net1.state_dict()\n",
    "    n_layers_old = net1.n_layers\n",
    "    n_head_old = net1.n_heads\n",
    "\n",
    "    dk_old = net1.d_model//net1.n_heads\n",
    "    dk_new = net2.d_model//net2.n_heads\n",
    "\n",
    "    new_state = net2.state_dict() \n",
    "    updated_state = deepcopy(new_state)\n",
    "    for k in new_state:\n",
    "        # print(k)\n",
    "        if k == \"position_encoding\" or k == \"self_attn_mask\":\n",
    "            continue\n",
    "        elif \"self_attn_norm\" in k.split(\".\") or \"ffn_norm\" in k.split(\".\"):\n",
    "            continue\n",
    "        elif \"attn_heads\" in k.split(\".\"):\n",
    "            updated_state[k] = th.zeros_like(new_state[k])\n",
    "            weight_name = k.split(\".\")\n",
    "            layer_idx = int(weight_name[2])\n",
    "            if layer_idx < n_layers_old:\n",
    "                head_idx = int(weight_name[5])   \n",
    "                lst = [(i//dk_old, i%dk_old) for i in (head_idx*dk_new, head_idx*dk_new +dk_new)]\n",
    "                w = []\n",
    "                if lst[0][0] == lst[1][0]:\n",
    "                    w.append(old_state[k][ lst[0][1]: lst[1][1], :])\n",
    "                else:\n",
    "                    for prev_head_idx in range(lst[0][0], lst[1][0]+1):\n",
    "                        if not prev_head_idx < n_head_old:\n",
    "                            continue\n",
    "                        weight_name_old = weight_name.copy()\n",
    "                        weight_name_old[5] = str(prev_head_idx)\n",
    "                        k_old = \".\".join(weight_name_old)\n",
    "\n",
    "                        if prev_head_idx == lst[0][0]:\n",
    "                            w_dash = old_state[k_old][lst[0][1]: , :]\n",
    "                            # print(rng,w_dash.shape)\n",
    "                            w.append(w_dash)\n",
    "\n",
    "                        elif prev_head_idx == lst[1][0]:\n",
    "                            w_dash = old_state[k_old][ :lst[1][1], :]\n",
    "                            # print(rng, w_dash.shape)\n",
    "                            w.append(w_dash)\n",
    "                        else:\n",
    "                            w.append(old_state[k_old])\n",
    "                    if w:\n",
    "                        final_old_w = th.cat(w)\n",
    "                        dice = [slice(dim) for dim in final_old_w.shape]\n",
    "                        updated_state[k][dice] = final_old_w\n",
    "        else:\n",
    "            updated_state[k] = th.zeros_like(new_state[k])\n",
    "            if k in old_state:\n",
    "                dice = [slice(dim) for dim in old_state[k].shape]\n",
    "                updated_state[k][dice] = old_state[k]\n",
    "        \n",
    "    net2.load_state_dict(updated_state)\n",
    "\n",
    "net2 = Transformer(n_layers=2, n_heads=4, d_model=16)\n",
    "knowledge_transfer(net2, \"./checkpoints/net1.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in updated_state:\n",
    "#     print(k, updated_state[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3194,  0.5154,  0.7834,  0.5756,  0.2514,  0.6421, -0.3060, -2.4690,\n",
       "           1.0348, -0.9342,  1.0119, -0.0656]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.3194,  0.5154,  0.7834,  0.5756,  0.2514,  0.6421, -0.3060, -2.4690,\n",
       "           1.0348, -0.9342,  1.0119, -0.0656,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor([11])\n",
    "em1 = net1.embed(x)\n",
    "em2 = net2.embed(x)\n",
    "em1, em2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.4170, -0.1111,  0.3454, -0.5814,  0.2336,  0.0348, -0.4388, -0.0494,\n",
       "           -0.1477,  0.4032,  0.4214,  0.0969]], grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []),\n",
       " (tensor([[-0.4170, -0.1111,  0.3454, -0.5814,  0.2336,  0.0348, -0.4388, -0.0494,\n",
       "           -0.1477,  0.4032,  0.4214,  0.0969,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "         grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.decoder.blocks[0].self_attn(em1,em1,em1), net2.decoder.blocks[0].self_attn(em2,em2,em2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-1.0726,  0.2880,  0.9357,  0.0589,  0.1053,  0.8309, -0.6380, -2.5448,\n",
       "            0.6212, -0.1655,  1.3562,  0.2249]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),\n",
       "  [],\n",
       "  []),\n",
       " (tensor([[-1.2233,  0.3458,  1.0938,  0.0832,  0.1330,  0.9774, -0.7215, -2.9246,\n",
       "            0.7293, -0.1784,  1.5777,  0.2721, -0.0411, -0.0411, -0.0411, -0.0411]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.decoder.blocks[0](em1), net2.decoder.blocks[0](em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3194,  0.5154,  0.7834,  0.5756,  0.2514,  0.6421, -0.3060, -2.4690,\n",
       "           1.0348, -0.9342,  1.0119, -0.0656,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " (tensor([[-0.4412,  0.5694,  0.8939,  0.6423,  0.2498,  0.7228, -0.4249, -3.0435,\n",
       "            1.1983, -1.1855,  1.1705, -0.1339, -0.0545, -0.0545, -0.0545, -0.0545]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em2, net2.decoder.blocks[1](em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[ 0.6222, -0.1207,  0.5258,  ...,  0.9854, -1.2260, -1.0245]],\n",
       "         grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []),\n",
       " (tensor([[ 0.7600, -0.0692,  0.5707,  ...,  1.2149, -1.4931, -1.2358]],\n",
       "         grad_fn=<MmBackward0>),\n",
       "  [],\n",
       "  []))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1(x), net2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = th.nn.LayerNorm(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3194,  0.5154,  0.7834,  0.5756,  0.2514,  0.6421, -0.3060, -2.4690,\n",
       "           1.0348, -0.9342,  1.0119, -0.0656,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.4412,  0.5694,  0.8939,  0.6423,  0.2498,  0.7228, -0.4249, -3.0435,\n",
       "           1.1983, -1.1855,  1.1705, -0.1339, -0.0545, -0.0545, -0.0545, -0.0545]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em2, norm(em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.decoder.blocks[0].self_attn_norm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40944/2350051098.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_w' is not defined"
     ]
    }
   ],
   "source": [
    "em2, F.layer_norm(em2, (16,), norm_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11b01b9c5b8ff60b99e90016c0fc35672e8bff0e840ae7e3fc812494c63e782d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
